\section*{Introduction générale}
\addcontentsline{toc}{section}{Introduction}

Ce article vous explique comment rendre les modèles d'apprentissage automatique (supervisés) interprétables. C'est pourquoi vous ne trouverez pas les méthodes les plus récentes et sophistiquées dans cet article, mais plutôt les méthodes établies et les concepts de base de l'interprétabilité en apprentissage automatique. Une manière de rendre l'apprentissage automatique interprétable est d'utiliser des modèles compréhensibles, tels que les modèles linéaires ou les arbres de décision. L'autre option est l'utilisation d'outils d'interprétation indépendants du modèle qui peuvent être appliqués à n'importe quel modèle d'apprentissage automatique supervisé et la dernière option est de regarder comment notre modèle se comporte sur des exemples spécifiques. Dans ce article nous aborderons donc ces trois options, leurs principaux outils, leurs avantages et leurs inconvénients. 

Enfin ce article est un travail de recherches et de lectures, il ne s'agit donc pas de travaux personnels mais d'un inventaire des travaux réalisés jusqu'alors. Ce article s'appuie en particulier sur le livre de Chritoph Molnar \hyperlink{https://christophm.github.io/interpretable-ml-book/}{ Interpretable Machine Learning}.
 
\subsubsection*{Qu'est-ce que l'apprentissage automatique ?}

Le Machine Learning (aussi appelé ML ou apprentissage automatique) regroupe un ensemble de méthodes que les ordinateurs utilisent pour créer et améliorer des prédictions ou des comportements basés sur des données. Ce article se concentre sur l'apprentissage automatique supervisé, qui englobe tous les problèmes de prédiction pour lesquels nous possédons un ensemble de données dont nous connaissons déjà le résultat d'intérêt. Des éléments tels que l'apprentissage par renforcement sont également exclus. Un inconvénient majeur de l'utilisation de Machine Learning est que les insights sur les données et la tâche que la machine résout sont cachés dans des modèles de plus en plus complexes. Il faut des millions de nombres (poids) pour décrire un réseau neuronal profond, et il n'y a aucun moyen de comprendre entièrement le modèle. Si vous vous concentrez uniquement sur la performance, vous obtiendrez automatiquement des modèles de plus en plus opaques.









